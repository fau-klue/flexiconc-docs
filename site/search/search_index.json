{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FlexiConc","text":"<p>This guide provides a comprehensive overview of FlexiConc, a Python package to streamline and enhance the computational analysis of concordances in corpus linguistic research.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>FlexiConc is a Python package developed to support corpus linguists by automating and simplifying the analysis of concordances\u2014lists of text segments centered around keywords or query matches. The package is informed by theoretical insights from the \u201cReading Concordances in the 21st Century\u201d research project, supported by the Arts and Humanities Research Council (AHRC, grant reference: AH/X002047/1) and the Deutsche Forschungsgemeinschaft (DFG, grant reference: 508235423).</p> <p>FlexiConc is built to facilitate a reproducible and accountable workflow in corpus research by systematically organizing, filtering, and interpreting concordance data.</p>"},{"location":"#what-is-flexiconc","title":"What is FlexiConc?","text":"<p>FlexiConc provides a suite of computational methods to flexibly manipulate concordance data, facilitating detailed pattern recognition and analysis.</p>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#concordance-views","title":"Concordance Views","text":"<p>FlexiConc allows the creation of concordance views, which are tailored representations of the subsets of the overall concordance.</p>"},{"location":"#core-analytical-strategies","title":"Core Analytical Strategies","text":"<p>FlexiConc is built around three fundamental strategies:</p> <ol> <li> <p>Selecting    Focus on specific subsets of concordance lines based on a variety of criteria, including metadata categories and contextual keywords.</p> </li> <li> <p>Ordering    Arrange concordance lines by sorting or ranking them, using numeric preference scores to prioritize those of interest.</p> </li> <li> <p>Grouping    Organize lines into meaningful clusters by applying explicit partitioning criteria or through clustering based on similarity measures.</p> </li> </ol>"},{"location":"#analysis-trees","title":"Analysis Trees","text":"<p>A crucial feature of FlexiConc is its analysis tree structure. Each node in this tree represents an operation (either a selection or a rearrangement) applied to the concordance data. This hierarchical record provides:</p> <ul> <li>Transparency: A complete history of the analytical steps performed.</li> <li>Reproducibility: Easy revisitation and validation of the analysis process.</li> <li>Documentation: A structured log that aids both in understanding and communicating the research methodology.</li> </ul>"},{"location":"#integration-and-extensibility","title":"Integration and Extensibility","text":"<p>FlexiConc is designed as a backend library to be seamlessly integrated into various concordancer applications. Its architecture enables:</p> <ul> <li>Seamless Integration: The host application manages corpus queries, initial concordance generation, and user interaction, while FlexiConc focuses on data processing and algorithmic analysis.</li> <li>Modular Design: New analytical algorithms can be added with minimal effort, ensuring the package remains adaptable to evolving research needs.</li> </ul>"},{"location":"#intended-users-and-use-cases","title":"Intended Users and Use Cases","text":"<p>FlexiConc is ideally suited for:</p> <ul> <li>Corpus Linguists: Those conducting both qualitative and quantitative analyses of linguistic patterns.</li> <li>Application Developers: Developers seeking robust backend solutions for concordance applications.</li> <li>Educators and Researchers: Individuals requiring systematic and reproducible documentation of their analytical workflows.</li> </ul> <p>To the users of FlexiConc, we recommend exploring the following sections for detailed guidance on using the package: - Loading Concordances: How to load concordance data from various sources. - Adding Nodes: How to add nodes to the analysis tree for further analysis. - [Noteb]</p>"},{"location":"adding_nodes/","title":"Adding Nodes in FlexiConc","text":"<p>FlexiConc organizes its analysis workflow using an analysis tree where each node represents either a subset of concordance lines or an arrangement (ordering and grouping) of those lines. The tree is expanded by applying user-defined combinations of concordances reading algorithms to the nodes in the tree, thus producing child or sibling nodes. Each node in the analysis tree corresponds to a concordance view which can be visualised by the host app. This document explains how to add nodes to the analysis tree using FlexiConc\u2019s API.</p>"},{"location":"adding_nodes/#overview","title":"Overview","text":"<p>There are two main types of nodes:</p> <ul> <li>Subset Nodes: Represent subsets of concordance lines obtained by applying a selection algorithm.</li> <li>Arrangement Nodes: Represent ordered and/or grouped views of concordance lines generated by applying sorting and grouping algorithms.</li> </ul> <p>Nodes are added to the analysis tree via methods provided on an existing node (usually starting from the root):</p> <ul> <li><code>add_subset_node(algo_tuple, keep_arrangement=True)</code></li> <li><code>add_arrangement_node(ordering=[], grouping=None)</code></li> </ul>"},{"location":"adding_nodes/#adding-subset-nodes","title":"Adding Subset Nodes","text":"<p>A subset node is created by applying a selection algorithm to an existing node. The selection algorithm is specified as a tuple:</p> <pre><code>(algorithm_name, args)\n</code></pre> <ul> <li>algorithm_name: A string identifying the selection algorithm (e.g., <code>\"Random Sample\"</code>, <code>\"Select by Metadata Attribute\"</code>).</li> <li>args: A dictionary of arguments required by the algorithm (e.g., <code>{'sample_size': 20, 'seed': 111}</code>).</li> </ul> <p>When you call <code>add_subset_node</code>, FlexiConc: 1. Checks if a sibling node with the same algorithm configuration exists. If so, it returns that node. 2. Otherwise, it executes the selection algorithm on the current node\u2019s concordance subset. 3. Creates a new subset node that contains the selected line IDs. 4. Optionally, it copies and restricts arrangement information from the parent node.</p> <p>Example:</p> <pre><code># Starting from the root node of the analysis tree, add a subset node\nsubset_node = c.root.add_subset_node(\n    (\"Random Sample\", {'sample_size': 20, 'seed': 111})\n)\n</code></pre> <p>This creates a new node that represents a random sample of 20 lines from the current subset.</p> <p>Another example using selecting by metadata attributes:</p> <pre><code>subset_node_long = c.root.add_subset_node(\n    (\"Select by Metadata Attribute\", {\n        'metadata_attribute': \"suspension_type\", \n        'value': \"long\",\n        'operator': \"==\",\n        'regex': False,\n        'case_sensitive': False,\n        'negative': False\n    })\n)\n</code></pre> <p>This node will include only those lines where the <code>suspension_type</code> is equal to <code>\"long\"</code>.</p>"},{"location":"adding_nodes/#adding-arrangement-nodes","title":"Adding Arrangement Nodes","text":"<p>An arrangement node provides a new view by ordering and/or grouping the concordance lines. This is done via:</p> <pre><code>add_arrangement_node(ordering=[], grouping=None)\n</code></pre> <ul> <li>ordering: A list of tuples. Each tuple has the form <code>(algorithm_name, args)</code>, where:</li> <li>algorithm_name is the name of an ordering algorithm (e.g., <code>\"Sort by Token-Level Attribute\"</code>).</li> <li> <p>args is a dictionary of parameters for that algorithm (e.g., <code>{'tokens_attribute': \"word\", 'sorting_scope': \"right\", 'reverse': True}</code>).</p> </li> <li> <p>grouping: An optional tuple of the form <code>(algorithm_name, args)</code> for a grouping algorithm (e.g., <code>(\"Partition by Ngrams\", {'tokens_attribute': \"pos\", 'positions': [-1], 'case_sensitive': True})</code>). If no grouping is needed, this can be set to <code>None</code>.</p> </li> </ul> <p>When you call <code>add_arrangement_node</code>, FlexiConc: 1. Executes each ordering algorithm and combines their sort keys to produce a final ordering. 2. If a grouping algorithm is provided, it applies the algorithm to partition the lines. 3. Creates a new arrangement node that contains the ordering result, grouping result (if any), and associated token span data.</p> <p>Example:</p> <pre><code>arrangement_node = c.root.add_arrangement_node(\n    ordering=[\n        (\"Sort by Token-Level Attribute\", {\n            'tokens_attribute': \"word\",\n            'sorting_scope': \"right\",\n            'reverse': True\n        })\n    ],\n    grouping=(\"Partition by Ngrams\", {\n        'tokens_attribute': \"pos\",\n        'positions': [-1],\n        'case_sensitive': True\n    })\n)\n</code></pre> <p>This example creates an arrangement node that orders the lines based on a token-level attribute (here, sorting by the right context of the token <code>\"word\"</code> in reverse order) and partitions the lines into groups based on the last token\u2019s part-of-speech (using an n-gram partitioning algorithm).</p> <p>As a Jupyter Notebook user, you will probably be interested in the interactive UI for adding nodes. See further documentation on Jupyter Notebook utilities.</p> <p>As a host app developer, you might want to implement your own visualizations for nodes. See the Concordance Views documentation for details on FlexiConc output used to represent nodes visually.</p>"},{"location":"concordance_views/","title":"Concordance Views","text":"<p>The <code>view()</code> method of an <code>AnalysisTreeNode</code> returns a dictionary representing a concordance view. This view is a structured, JSON-serializable summary of the concordance data at a given node in the analysis tree. The output is organized into several keys, each providing specific information about the concordance lines and any additional processing (ordering, grouping, ranking, token spans) that has been applied.</p> <p>Below is the exact specification of the output:</p>"},{"location":"concordance_views/#1-selected_lines","title":"1. <code>selected_lines</code>","text":"<ul> <li>Type: <code>List[int]</code></li> <li>Description:   A list of line numbers (indices from the original concordance metadata) that are visible in the view.<ul> <li>If the node defines its own <code>selected_lines</code>: That list is used.</li> <li>Otherwise: The view inherits <code>selected_lines</code> from the nearest ancestor that has them.</li> <li>Fallback: If no such ancestor exists, it defaults to all line numbers in the concordance metadata.</li> </ul> </li> </ul>"},{"location":"concordance_views/#2-ordering","title":"2. <code>ordering</code>","text":"<ul> <li>Type: <code>List[int]</code></li> <li>Description:   An ordered list of the visible line numbers. This ordering is determined by:<ul> <li>The node\u2019s own <code>ordering_result[\"sort_keys\"]</code>, if present.</li> <li>Or, inherited from the nearest ancestor that has an ordering result.</li> </ul> </li> <li>Filtering: Only line numbers present in the current node\u2019s <code>selected_lines</code> are included.</li> <li>Default: If no ordering is defined, it falls back to the natural order (using the line number as the sort key).</li> </ul>"},{"location":"concordance_views/#3-grouping-optional","title":"3. <code>grouping</code> (Optional)","text":"<ul> <li>Type: <code>dict</code></li> <li>Description:\u00a0Included only when a grouping / clustering algorithm has been applied to the node.\u00a0The object bundles column metadata and the actual list / tree of groups.</li> </ul> <pre><code>\"grouping\": {\n  \"column_info\": [          \n    {\"name\": \"Quality\", \"type\": \"float\", \"description\": \"Silhouette score\", ...},\n    {\"name\": \"Size\",    \"type\": \"int\",   \"description\": \"Number of lines\",   ...}\n  ],\n  \"partitions\": [           // or \"clusters\" for hierarchical output\n    {\n      \"id\": 0,\n      \"label\": \"Cluster_0\",\n      \"line_ids\": [ ... ],    // ordered according to the global ordering\n      \"prototypes\": [ ... ],  // optional\n      \"info\": {               // keys correspond to column_info\n        \"Quality\": 0.72,\n        \"Size\": 37\n      }\n    },\n    {\n      \"id\": 1,\n      \"label\": \"Cluster_1\",\n      \"line_ids\": [ ... ],\n      \"info\": {\n        \"Quality\": 0.65,\n        \"Size\": 22\n      }\n    }\n  ]\n}\n</code></pre> <ul> <li><code>grouping</code> (object) </li> <li><code>column_info</code> \u2013 <code>List[dict]</code> describing each supplementary group\u2011level metric.  </li> <li> <p><code>partitions</code> or <code>clusters</code> \u2013 <code>List[Group]</code>; flat for partitions or hierarchical when using clusters.</p> </li> <li> <p><code>Group</code> (object) </p> </li> <li><code>id</code> \u2013 <code>int</code>, required.  </li> <li><code>label</code> \u2013 <code>str</code>, optional display name.  </li> <li><code>line_ids</code> \u2013 <code>List[int]</code>, required for leaf groups (ordered by the view\u2019s <code>ordering</code>).  </li> <li><code>prototypes</code> \u2013 <code>List[int]</code>, optional representative lines.  </li> <li><code>info</code> \u2013 <code>Dict[str,\u00a0Any]</code> keyed by the entries in <code>column_info</code>.  </li> <li><code>children</code> \u2013 <code>List[Group]</code>, only present for hierarchical clusterings.</li> </ul>"},{"location":"concordance_views/#31-column_info-column_info","title":"3.1\u00a0<code>column_info</code> <code>column_info</code>","text":"<p>Lists group\u2011level columns (e.g. cluster quality, within\u2011variance, size).</p>"},{"location":"concordance_views/#32-partitions-clusters","title":"3.2\u00a0<code>partitions</code>\u00a0/\u00a0<code>clusters</code>","text":"<p>A flat list (for partitions) or a recursive list (for clusters). Each dict can contain:</p> Key Type Always? Meaning <code>id</code> <code>int</code> \u2714\ufe0e Numeric identifier. <code>label</code> <code>str</code> \u2716\ufe0e Human\u2011readable name to be shown in UI. <code>line_ids</code> <code>List[int]</code> \u2714\ufe0e for partitions / leaf clusters Lines that belong to this group. Ordered according to the view\u2019s <code>ordering</code>. <code>prototypes</code> <code>List[int]</code> \u2716\ufe0e Representative line\u2011ids. <code>info</code> <code>Dict[str,Any]</code> \u2714\ufe0e (may be empty) Values keyed by <code>column_info[i][\"name\"]</code>. <code>children</code> <code>List[dict]</code> \u2716\ufe0e Present only for hierarchical clustering; same structure recursively."},{"location":"concordance_views/#4-global_info-optional","title":"4. <code>global_info</code> (Optional)","text":"<ul> <li>Type: <code>Dict[str, Any]</code></li> <li>Description:   A dictionary containing overall information about the view. For example:<ul> <li>Differentiation information from the ordering algorithm (e.g., counts of adjacent line pairs that were differentiated by each ordering algorithm).</li> <li>Any additional information stored in the node (from <code>self.info</code>).</li> </ul> </li> </ul>"},{"location":"concordance_views/#5-line_info-optional","title":"5. <code>line_info</code>  (Optional)","text":"<p>This part of a concordance view typically includes ranking scores.</p> <pre><code>\"line_info\": {\n  \"column_info\": [ /* array of column metadata */ ],\n  \"data\": {\n    ...,\n    12: {               // line_id\n      \"Ranking: KWIC Grouper Ranker\": 1,\n      \"Ranking: GDEX\": 0.485\n    },\n    13: {               // line_id\n      \"Ranking: KWIC Grouper Ranker\": 0,\n      \"Ranking: GDEX\": 0.871\n    },\n    ...\n  }\n}\n</code></pre>"},{"location":"concordance_views/#51-column_info","title":"5.1 <code>column_info</code>","text":"<p>Each object fully describes one column.</p> Field Type Description <code>key</code> <code>str</code> The human\u2011readable column name. Convention: <code>\"Ranking: &lt;Algorithm name&gt;\"</code>. <code>algorithm</code> <code>str</code> The exact name of the algorithm used for ranking. <code>algorithm_index_withing_ordering</code> <code>int</code> The position of the algorithm within the list of ordering algorithms used at current node (0-based). <code>type</code> <code>str</code> <code>\"ranking\"</code> for ranking algorithms. <code>description</code> <code>str</code> One\u2011line tooltip explaining the column."},{"location":"concordance_views/#52-data","title":"5.2 <code>data</code>","text":"<p>Keys \u2192 <code>line_id</code>. Values \u2192 dict mapping column key \u2192 line information, most typically ranking value (<code>int</code> or <code>float</code>). Only lines listed in <code>selected_lines</code> are included.</p>"},{"location":"concordance_views/#6-token_spans-optional","title":"6. <code>token_spans</code> (Optional)","text":"<ul> <li>Type: <code>List[dict]</code></li> <li>Description:   A list of token span objects used to mark tokens in a KWIC (Key Word In Context) display. Each token span dictionary includes:<ul> <li><code>line_id</code>: The line number in which the span occurs.</li> <li><code>start_id_in_line</code>: The starting token id (inclusive) relative to the line. </li> <li><code>end_id_in_line</code>: The ending token id (inclusive) relative to the line.</li> <li><code>category</code>: A string indicating the mark (e.g., <code>\"A\"</code>).</li> <li><code>weight</code>: A numerical weight, typically in the range <code>[0, 1]</code>.</li> </ul> </li> </ul>"},{"location":"concordance_views/#7-node_type","title":"7. <code>node_type</code>","text":"<ul> <li>Type: <code>str</code></li> <li>Description:   A string indicating the type of the node (e.g., <code>\"subset\"</code>, <code>\"arrangement\"</code>).</li> </ul>"},{"location":"concordance_views/#additional-notes","title":"Additional Notes","text":"<ul> <li> <p>Serialization:   The entire view is designed to be JSON-serializable.</p> </li> <li> <p>Mandatory vs. Optional: </p> <ul> <li>The keys <code>selected_lines</code> and <code>ordering</code> are always present.</li> <li>The keys <code>grouping</code>, <code>global_info</code>, <code>line_info</code>, and <code>token_spans</code> are optional and are included only if relevant algorithms (grouping, ranking, etc.) have been applied to the node.</li> </ul> </li> <li> <p>Ordering Details:   The ordering list sorts the lines based on the sort keys computed by ordering algorithms. Ties are handled in a stable manner to maintain consistency.</p> </li> <li> <p>Token Spans:   When present, token spans provide precise information for marking specific segments of tokens within each line, enhancing the KWIC display for further visualization.</p> </li> </ul> <p>This specification outlines the complete structure and content of a Concordance View as generated by FlexiConc. Use it as a reference for understanding the output and for integrating or visualizing concordance data in your applications.</p>"},{"location":"flexiconc_for_developers/","title":"FlexiConc for developers","text":"<p>FlexiConc is intended for developers who want to integrate advanced concordance analysis capabilities into their corpus management systems. The package is designed to be modular and extensible, allowing developers to add new features and algorithms as needed.</p> <p>The host app can pass a concordance to FlexiConc, which will then process the data and return the results. This allows for a clear separation of concerns, where the host app handles user interaction and corpus management, while FlexiConc focuses on the analytical tasks. FlexiConc also takes care of the analysis tree, which records all operations performed on the concordance data, ensuring transparency and reproducibility.</p>"},{"location":"loading_concordances/","title":"Loading Concordances","text":""},{"location":"loading_concordances/#1-utility-functions-for-loading-data-from-external-systems","title":"1. Utility Functions for Loading Data from External Systems","text":"<p>FlexiConc provides a set of utility functions to retrieve concordance data from external corpus systems. These functions automatically build and populate the internal DataFrames (metadata, tokens, and matches) by processing raw query results. The following sections describe each retrieval function in detail, including the required arguments and example calls.</p>"},{"location":"loading_concordances/#11-retrieving-from-cwb","title":"1.1 Retrieving from CWB","text":"<p>The function to retrieve data from IMS Open Corpus Workbench (CWB) requires the following arguments:</p> <ul> <li>registry_dir (str):   The path to the CWB registry directory.</li> <li>corpus_name (str):   The name of the corpus to be queried.</li> <li>query (str):   The search query string.</li> <li>tokens_attrs (list of str):   A list of token-level attribute names to retrieve (e.g., <code>[\"word\", \"lemma\", \"pos\"]</code>).</li> <li>metadata_attrs (list of str):   A list of metadata attribute names to retrieve (e.g., <code>[\"speaker\", \"chapter\", \"paragraph\"]</code>).</li> <li>context (int):   The number of tokens to include as context on each side of the matched token.</li> </ul> <p>Example:</p> <pre><code>concordance.retrieve_from_cwb(\n    registry_dir=\"/path/to/cwb/registry\",\n    corpus_name=\"my_corpus\",\n    query=\"search_term\",\n    tokens_attrs=[\"word\", \"lemma\", \"pos\"],\n    metadata_attrs=[\"speaker\", \"chapter\", \"paragraph\"],\n    context=20\n)\n</code></pre> <p>Internally, this function performs the following steps: 1. Instantiates a CWB Corpus object using the specified registry directory and corpus name. 2. Executes the query with the provided context. 3. Converts the raw output into DataFrames for metadata, tokens, and, if necessary, matches by unnesting token-level data and computing token positions. 4. Populates the Concordance object with these DataFrames.</p>"},{"location":"loading_concordances/#12-retrieving-from-clic","title":"1.2 Retrieving from CLiC","text":"<p>The retrieval function for CLiC is designed for corpora accessible via the CLiC API. It accepts the following arguments:</p> <ul> <li>query (list of str):   A list containing one or more query strings.</li> <li>corpora (str):   The identifier(s) for the corpus or corpora to search.</li> <li>subset (str):   A string specifying which subset of the corpus to search (e.g., <code>\"all\"</code>, <code>\"quote\"</code>, or <code>\"nonquote\"</code>).</li> <li>contextsize (int):   The number of tokens of context to include on each side of the match.</li> </ul> <p>Example:</p> <pre><code>concordance.retrieve_from_clic(\n    query=[\"search term\"],\n    corpora=\"my_corpora\",\n    subset=\"all\",\n    contextsize=20\n)\n</code></pre> <p>This function works as follows: 1. Sends HTTP GET requests to the CLiC API with the specified parameters. 2. Parses the JSON response to extract both metadata and token-level details. 3. Aggregates tokens from nested JSON structures and assigns proper <code>line_id</code> values. 4. Constructs the internal DataFrames and assigns them to the Concordance object.</p>"},{"location":"loading_concordances/#13-retrieving-from-sketchengine","title":"1.3 Retrieving from SketchEngine","text":"<p>The SketchEngine retrieval function is used to retrieve concordance data via the SketchEngine API. It requires these arguments:</p> <ul> <li>query (str):   The search query string.</li> <li>corpus (str):   The identifier for the target corpus (e.g., <code>\"bnc2\"</code>).</li> <li>api_username (str):   Your SketchEngine username.</li> <li>api_key (str):   Your SketchEngine API key.</li> </ul> <p>Example:</p> <pre><code>concordance.retrieve_from_sketchengine(\n    query=\"search_term\",\n    corpus=\"bnc2\",\n    api_username=\"your_username\",\n    api_key=\"your_api_key\"\n)\n</code></pre> <p>This function operates as follows: 1. Constructs an authenticated HTTP request using your API credentials. 2. Retrieves the JSON response containing both token-level and structural data. 3. Unnests the token data and maps structural details to create the metadata, tokens, and matches DataFrames. 4. Computes additional columns such as <code>id_in_line</code> if they are not provided. 5. Assigns these DataFrames to the Concordance object.</p> <p>Add this after the section on \u201cUtility Functions for External Systems\u201d:</p>"},{"location":"loading_concordances/#14-loading-from-cqpweb-export","title":"1.4 Loading from CQPweb Export","text":"<p>FlexiConc provides a convenient function for loading concordance data exported from CQPweb. This function automatically parses standard CQPweb concordance TSV files (with or without POS tags) and constructs the required internal DataFrames (metadata, tokens, matches) for analysis.</p> <p>Function: <code>Concordance.load_from_cqpweb_export(filepath)</code></p> <p>Description: This method reads a CQPweb concordance export (TSV format), handling both plain and POS-tagged exports. Tagged columns (such as \u201cTagged context before\u201d) are detected automatically. POS information is separated into a distinct column if present. The method also collects metadata fields from all non-token columns.</p> <p>Example:</p> <pre><code>concordance.load_from_cqpweb_export(\"path/to/cqpweb_export.tsv\")\n</code></pre> <ul> <li>filepath: Path to the CQPweb concordance TSV file (exported from the CQPweb interface).</li> </ul> <p>Each of these retrieval functions converts external data formats into FlexiConc\u2019s standard internal structure, allowing you to analyze and visualize your corpus data immediately after loading.</p>"},{"location":"loading_concordances/#2-loading-data-from-external-flexiconc-compatible-files","title":"2. Loading Data From External FlexiConc-Compatible Files","text":"<p>FlexiConc\u2019s <code>Concordance.load()</code> method is used to populate the three internal DataFrames\u2014metadata, tokens, and matches\u2014from external data files. The method is designed to accept TSV (Tab-Separated Values) files based on the MTSV format proposed by Anthony &amp; Evert (2019).</p>"},{"location":"loading_concordances/#concordanceload-method","title":"Concordance.load() Method","text":"<p>The <code>load()</code> method accepts file paths for: - metadata: A TSV file containing structural information about each line. - tokens: A TSV file containing token-level data. - matches (optional): A TSV file containing match location details if your tokens file does not provide an <code>offset</code> column. - info (optional): A JSON file or dictionary with additional information.</p> <p>The method reads these files using Pandas and assigns the resulting DataFrames to the Concordance object\u2019s attributes. It automatically checks for the presence of required columns such as <code>line_id</code> in the metadata file. In the tokens file, if the <code>id_in_line</code> column is missing, the method reconstructs it based on the token order. Similarly, if the tokens file includes an <code>offset</code> column, the matches table is optional; otherwise, you must provide a matches table to specify the match location.</p>"},{"location":"loading_concordances/#example","title":"Example","text":"<p>Assuming you have TSV files with headers, you can load them as follows:</p> <pre><code>from flexiconc.concordance import Concordance\n\n# Create the Concordance object\nconcordance = Concordance()\n\n# Option A: Tokens file includes 'offset'\nconcordance.load(\n    metadata=\"path/to/metadata.tsv\",  # must contain 'line_id' and other structural columns\n    tokens=\"path/to/tokens.tsv\",      # must contain 'line_id' and 'offset' (id_in_line is optional)\n    info=\"path/to/info.json\"          # optional additional information\n)\n\n# Option B: Tokens file does NOT include 'offset'; matches provided separately\nconcordance.load(\n    metadata=\"path/to/metadata.tsv\",   # must contain 'line_id'\n    tokens=\"path/to/tokens.tsv\",       # must contain 'line_id' (other token attributes as needed)\n    matches=\"path/to/matches.tsv\",     # must contain 'line_id', 'match_start', 'match_end', and 'slot'\n    info=\"path/to/info.json\"           # optional additional information\n)\n</code></pre>"},{"location":"loading_concordances/#technical-details-and-options","title":"Technical Details and Options","text":"<ul> <li>TSV Files with Headers:   The load method assumes that each TSV file has a header row. This header is used to name the columns of the DataFrame. For example, the metadata TSV should include a header line like:</li> </ul> <pre><code>line_id   text_id   chapter   paragraph   sentence\n</code></pre> <p>Similarly, the tokens TSV must have headers for columns such as <code>line_id</code>, <code>offset</code>, and optionally <code>id_in_line</code>, along with token attributes like <code>word</code>, <code>pos</code>, etc.</p> <ul> <li> <p>Providing Offsets vs. Matches Table:</p> <ul> <li>With Offsets:   If your tokens file includes an <code>offset</code> column, the load method uses it to determine the token\u2019s relative position (negative for left context, zero for the matched token, positive for right context). In this configuration, you do not need to supply a matches table.  </li> <li> <p>Without Offsets:   If the tokens file lacks an <code>offset</code> column, you must provide a matches table. The matches file should include:</p> <ul> <li><code>line_id</code>: To link the match to the correct metadata entry.</li> <li><code>match_start</code> and <code>match_end</code>: Token indices that mark the beginning and end of the match.</li> </ul> <p>It can also include:</p> <ul> <li><code>slot</code>: An integer value that is used to determine the focus of concordance operations if a line contains multiple matches.</li> </ul> </li> </ul> </li> <li> <p>Omission of <code>id_in_line</code>:   The tokens file may or may not include an <code>id_in_line</code> column. If it is omitted, FlexiConc will reconstruct it by ordering the tokens within each line based on their original position.</p> </li> <li> <p>Internal Consistency:   Regardless of which option you choose, the <code>load()</code> method guarantees that the Concordance object will have all three internal DataFrames:</p> <ul> <li>metadata: Contains structural data.</li> <li>tokens: Contains token-level data.</li> <li>matches: Contains match location data (either directly provided or inferred from the tokens).</li> </ul> </li> </ul> <p>By providing the data in these formats, FlexiConc ensures that all subsequent analyses\u2014such as building an analysis tree or applying algorithms\u2014work on a consistent and standardized dataset.</p>"},{"location":"loading_texts/","title":"Loading Texts to Generate Concordances","text":"<p>This document explains how to load your own texts into FlexiConc for generating concordances. Currently, FlexiConc supports loading raw texts and Wmatrix corpora, using the <code>TextImport</code> interface.</p>"},{"location":"loading_texts/#1-loading-plain-texts-with-textimport","title":"1. Loading Plain Texts with <code>TextImport</code>","text":"<p>FlexiConc's <code>TextImport</code> class allows you to load and tokenize your own text files, build a searchable SQLite database, and run queries producing FlexiConc concordances.</p>"},{"location":"loading_texts/#11-database-structure","title":"1.1. Database Structure","text":"<p>A <code>TextImport</code> SQLite database contains:</p> <ul> <li> <p>tokens table: One row per token, with at least:</p> </li> <li> <p><code>cpos</code> (corpus position, integer, unique)</p> </li> <li><code>word</code> (surface form, string)</li> <li> <p>Optional: <code>lemma</code>, <code>pos</code>, etc.</p> </li> <li> <p>spans_* tables: Boundaries of token spans, such as sentences/files. Examples:</p> </li> <li> <p><code>spans_s</code>: Sentence spans, columns: <code>id</code>, <code>start</code>, <code>end</code>.</p> </li> <li><code>spans_file</code>: File/document spans, columns: <code>id</code>, <code>start</code>, <code>end</code>.</li> </ul> <p>Span tables can contain any optional columns, such as <code>filename</code> for <code>spans_file</code>.</p> <p>You can add custom span types as needed.</p>"},{"location":"loading_texts/#12-creating-a-database-from-raw-files","title":"1.2. Creating a Database from Raw Files","text":"<p>Use <code>TextImport.load_files</code> to tokenize plain text files (or directories), split into sentences, and build all necessary tables:</p> <pre><code>from flexiconc import TextImport\n\nti = TextImport()  # Creates a temporary in-memory DB\n\nti.load_files(\n    paths=[\"texts/file1.txt\", \"texts/file2.txt\"],  # List of text files or folders\n    db_name=\"mycorpus.sqlite\",                     # Save DB to disk (optional)\n    use_spacy=False,                               # Use regex tokenization rules (or set True for spaCy)\n    spacy_model=\"en_core_web_sm\",\n    lemma=False,                                   # Add lemma using spaCy\n    pos=False,                                     # Add part-of-speech using spaCy\n    tag=False                                      # Add detailed POS tags using spaCy\n)\n</code></pre>"},{"location":"loading_texts/#notes","title":"Notes","text":"<ul> <li>Accepts files or directories (recursively scans folders).</li> <li>Supports basic regex sentence splitting or spaCy-based segmentation and annotation.</li> </ul>"},{"location":"loading_texts/#13-building-concordances-from-queries","title":"1.3. Building Concordances from Queries","text":"<p>Once your corpus is loaded, you can search it and build concordances in a single step using <code>concordance_from_query</code>:</p> <pre><code># Run a search and build a FlexiConc-style concordance object in one call\nconc = ti.concordance_from_query(\n    query=\"climate change\",           # Plain text or CQP-style search string\n    context_size=(20, 20),              # (left, right) context window size\n    limit_context_span=\"s\",           # Limit context to a sentence (or use \"file\" for file/document); leave empty for no limiting by span\n    span_types_for_metadata=['s', 'file']  # Which spans to include as metadata (optional)\n)\n\n---\n\n## 2. Loading Wmatrix Corpora\n\nFor corpora hosted in Wmatrix, FlexiConc provides a streamlined import function. The **recommended entry point** is the `load` function.\n\n```python\nfrom flexiconc.utils import wmatrix\n\nti = wmatrix.load(\n    corpus_name=\"LabourManifesto2005\",   # Wmatrix corpus name (as in the web interface)\n    username=\"your_username\",\n    password=\"your_password\",\n    db_filename=\"labour2005.db\"           # Local file to store the database\n)\n</code></pre> <p>If you have a Wmatrix corpus in a local SQLite database, you can load it directly:</p> <pre><code>from flexiconc.utils import wmatrix\n\nti = wmatrix.load(\n    db_filename=\"labour2005.db\"           # Local file where the database is stored\n)\n</code></pre>"},{"location":"loading_texts/#22-using-the-textimport-api","title":"2.2. Using the TextImport API","text":"<p>Once loaded, <code>ti</code> works just like for plain text:</p> <pre><code># Run a concordance query\nconc = ti.concordance_from_query(\n    query=\"antisocial behaviour\",           # Plain text or CQP-style search string\n    context_size=(20, 20),              # (left, right) context window size\n    limit_context_span=\"s\",           # Limit context to a sentence (or use \"file\" for file/document); leave empty for no limiting by span\n    span_types_for_metadata=['s', 'file']  # Which spans to include as metadata (optional)\n)\n</code></pre>"},{"location":"notebook_utils/","title":"Jupyter Notebook Utilities","text":"<p>This page documents the most important interactive utilities for working with the FlexiConc analysis tree and concordance objects in Jupyter notebooks.</p>"},{"location":"notebook_utils/#add_node_uiparent-executetrue","title":"<code>add_node_ui(parent, *, execute=True)</code>","text":"<p>Interactively add a subset or arrangement node to an analysis tree.</p> <ul> <li> <p>Parameters:</p> </li> <li> <p><code>parent</code> (<code>AnalysisTreeNode</code>):     The node to which a new child node will be attached.</p> </li> <li> <p><code>execute</code> (<code>bool</code>, default <code>True</code>):     If <code>True</code>, the node is created immediately.     If <code>False</code>, the code snippet for creating the node is generated and inserted into a new cell for review or manual execution.</p> </li> <li> <p>Returns:   A handle for <code>AnalysisTreeNode</code>.</p> </li> <li> <p>After the OK button is pressed, the handle provides access to the created node object (if <code>execute=True</code>)</p> </li> <li> <p><code>.code</code> \u2014 the generated Python code for the operation</p> </li> <li> <p>How it works:   Displays a user interface with:</p> </li> <li> <p>Choice of node type: \"subset\" or \"arrangement\"</p> </li> <li>Configuration forms for algorithms and their parameters</li> <li>OK button: runs or generates the code, shows output, errors, or the code snippet below the widget</li> </ul> <p>Example usage:</p> <pre><code>handle = add_node_ui(tree.root)\n# \u2192 Interactively select node type and parameters, then press OK.\n# handle references the newly created node (after creation)\n# handle.code gives the generated code\n</code></pre>"},{"location":"notebook_utils/#add_annotation_uiconcordance","title":"<code>add_annotation_ui(concordance)</code>","text":"<p>Interactively add an annotation to a Concordance object.</p> <ul> <li> <p>Parameters:</p> </li> <li> <p><code>concordance</code> (<code>Concordance</code>):     The concordance object to annotate.</p> </li> <li> <p>Behavior:</p> </li> <li> <p>Shows a widget for selecting an annotation algorithm and specifying its arguments.</p> </li> <li>Lets you (optionally) specify column names for the annotation.</li> <li> <p>Press Annotate to run the annotation and see results or errors below.</p> </li> <li> <p>Output:</p> </li> <li> <p>Applies the annotation and prints the code that was executed.</p> </li> </ul> <p>Example usage:</p> <pre><code>add_annotation_ui(conc)\n# \u2192 Use the form to select the annotation, set arguments, specify columns, and annotate interactively.\n</code></pre>"},{"location":"notebook_utils/#show_kwicnode-n0-n_groupsnone-token_attrword-extra_token_attrsnone-metadata_columnsnone-height600","title":"<code>show_kwic(node, n=0, n_groups=None, token_attr='word', extra_token_attrs=None, metadata_columns=None, height=600)</code>","text":"<p>Display a KWIC (Key Word in Context) table for a node in a scrollable notebook cell.</p> <ul> <li> <p>Parameters:</p> </li> <li> <p><code>node</code> (<code>AnalysisTreeNode</code>):     The node (subset or arrangement) whose data you want to view.</p> </li> <li><code>n</code> (<code>int</code>, default <code>0</code>):     Maximum number of lines per group (or in total if not grouped). If <code>0</code>, all lines are shown.</li> <li><code>n_groups</code> (<code>int</code>, optional):     Maximum number of groups to show (if partitioned).</li> <li><code>token_attr</code> (<code>str</code>, default <code>'word'</code>):     Token attribute to display in the table.</li> <li><code>extra_token_attrs</code> (<code>list</code>, optional):     Additional token attributes to display as subscripts.</li> <li><code>metadata_columns</code> (<code>list</code>, optional):     Metadata columns to show per line.</li> <li> <p><code>height</code> (<code>int</code>, default <code>600</code>):     Height of the KWIC display area in pixels. Set to <code>0</code> for no scroll.</p> </li> <li> <p>How it works:   Renders a scrollable HTML KWIC table using the node's data and chosen display settings.</p> </li> </ul> <p>Example usage:</p> <pre><code>show_kwic(node, n=50, token_attr='lemma', metadata_columns=['genre'])\n</code></pre>"},{"location":"notebook_utils/#show_analysis_treeconcordance-suppress_line_infotrue-marknone-list_annotationsnone","title":"<code>show_analysis_tree(concordance, suppress_line_info=True, mark=None, list_annotations=None)</code>","text":"<p>Display the analysis tree structure for a Concordance object.</p> <ul> <li> <p>Parameters:</p> </li> <li> <p><code>concordance</code> (<code>Concordance</code>):     The object whose analysis tree will be visualized.</p> </li> <li><code>suppress_line_info</code> (<code>bool</code>, default <code>True</code>):     If <code>True</code>, hides detailed line info in the display.</li> <li><code>mark</code> (optional):     Optionally, specify the id of the node to highlight.</li> <li> <p><code>list_annotations</code> (<code>bool</code>, optional):     If <code>True</code>, lists annotations in the display.</p> </li> <li> <p>How it works:</p> </li> <li> <p>Produces an HTML visualization of the analysis tree and displays it in the notebook.</p> </li> </ul> <p>Example usage:</p> <pre><code>show_analysis_tree(conc)\n</code></pre>"},{"location":"representing_concordances/","title":"Representing Concordances","text":"<p>FlexiConc is designed to standardize the way concordance data is stored and processed. Internally, a Concordance object always contains three Pandas DataFrames\u2014metadata, tokens, and matches\u2014which together form its internal structure.</p>"},{"location":"representing_concordances/#1-internal-structure-of-the-concordance-object","title":"1. Internal Structure of the Concordance Object","text":"<p>The Concordance object maintains three DataFrames that hold different aspects of the concordance data:</p>"},{"location":"representing_concordances/#a-metadata-dataframe","title":"A. Metadata DataFrame","text":"<p>The <code>metadata</code> DataFrame stores information about each concordance line. It must include a unique identifier, line_id, for each row. Additional columns may include details such as the source text identifier, chapter, paragraph, and sentence.</p> <p>For example, the metadata table might be structured as follows:</p> line_id text_id chapter paragraph sentence 0 ED 10 35 63 1 ED 10 36 71 2 ED 14 115 262 3 ED 23 7 78 4 LD 45 85 192"},{"location":"representing_concordances/#b-tokens-dataframe","title":"B. Tokens DataFrame","text":"<p>The tokens DataFrame holds token-level details for each concordance line. It must include line_id to link tokens to their corresponding metadata entry. In addition, the tokens DataFrame typically contains:</p> <ul> <li> <p>offset: Indicates the token\u2019s relative position to the matched (node) token(s):</p> <ul> <li>Negative values represent tokens in the left context.</li> <li>Zero marks the matched (node) token(s).</li> <li>Positive values represent tokens in the right context.</li> </ul> </li> <li> <p>id_in_line: (Optional) The token\u2019s sequential index within the line. If omitted, FlexiConc will reconstruct it.</p> </li> <li>word: The token text.</li> <li>Other attributes such as part-of-speech, lemma, etc., may also be provided.</li> </ul> <p>A tokens table might look like this:</p> cpos offset word pos lemma line_id id_in_line 445643 -20 to IN to 0 0 445644 -19 the DT the 0 1 445645 -18 toll-keeper JJ toll-keeper 0 2 445646 -17 keeper NN keeper 0 3 445647 -16 . . . 0 4 445648 -15 then RB then 0 5 445649 -14 he PRP he 0 6 <p>Notes: - The cpos column (optional) represents the corpus position in the original corpus when available.</p>"},{"location":"representing_concordances/#c-matches-dataframe","title":"C. Matches DataFrame","text":"<p>The <code>matches</code> DataFrame specifies the match location within each of the concordance lines. In simple cases, it contains the same information as the offset column in <code>tokens</code>. It must include:</p> <ul> <li>line_id: To associate the match with its corresponding metadata row.</li> <li>match_start and match_end: Indicate the token indices that mark the beginning and end of the match.</li> <li>slot: An integer that allows for multiple matches per line.</li> </ul> <p>An example <code>matches</code> table might look like this:</p> line_id match_start match_end slot 0 20 20 0 1 61 61 0 2 102 102 0 3 143 143 0 <p>The <code>matches</code> DataFrame is essential when there are multiple matching slots per line:</p> line_id match_start match_end slot 0 20 20 0 0 22 22 1 1 61 61 0 1 62 62 1 <p>The <code>slot</code> column allows FlexiConc to focus on different matches within the same line, enabling more complex analyses.</p>"},{"location":"algorithms/all_algorithms/","title":"File: select_rank_wrapper.py","text":""},{"location":"algorithms/all_algorithms/#select_by_rank","title":"<code>select_by_rank</code>","text":"<p>Selects lines based on rank values obtained from a selected 'algo_*' key in the ordering_result[\"rank_keys\"] of the active_node, using a comparison operator and value.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters:     conc (Union[Concordance, ConcordanceSubset]): The concordance or subset of data.     args (dict): Arguments include:         - active_node (object): The active node containing the ordering_result with rank_keys.         - algo_key (str): The specific algorithm key from ordering_result[\"rank_keys\"] to use.                           Allowed values are those that start with \"algo_\". By default, the smallest key (lowest number) is used.         - comparison_operator (str): The comparison operator ('==', '&lt;=', '&gt;=' ,'&lt;', '&gt;'). Default is \"==\".         - value (number): The value to compare the rank keys against. Default is 0.</p> <p>Returns:     dict: A dictionary containing:         - \"selected_lines\": A sorted list of selected line IDs.         - \"line_count\": The total number of selected lines.</p>"},{"location":"algorithms/all_algorithms/#file-select_randompy","title":"File: select_random.py","text":""},{"location":"algorithms/all_algorithms/#select_random","title":"<code>select_random</code>","text":"<p>Selects a random sample of line IDs from the concordance metadata.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it. - **kwargs: Arguments defined dynamically in the schema.</p> <p>Returns: - dict: A dictionary containing:     - \"selected_lines\": A list of randomly selected line IDs.     - \"line_count\": The number of selected lines.</p>"},{"location":"algorithms/all_algorithms/#file-sort_by_corpus_positionpy","title":"File: sort_by_corpus_position.py","text":""},{"location":"algorithms/all_algorithms/#sort_by_corpus_position","title":"<code>sort_by_corpus_position</code>","text":"<p>Sorts the concordance or subset of data by line_id, which corresponds to the corpus position.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it. - args (dict): Arguments include:     - No additional arguments are required for this function.</p> <p>Returns: - dict: A dictionary containing:     - \"sort_keys\": A mapping from line IDs to their sorted positions.</p>"},{"location":"algorithms/all_algorithms/#file-annotate_spacy_pospy","title":"File: annotate_spacy_pos.py","text":""},{"location":"algorithms/all_algorithms/#annotate_spacy_pos","title":"<code>annotate_spacy_pos</code>","text":"<p>Annotates tokens with spaCy part-of-speech (POS) tags or related attributes. This algorithm uses spaCy to determine the tag information for each token in the specified token attribute. The spacy_attributes argument is always treated as a list: even if a single attribute is desired, it should be provided as a one-element list. The algorithm returns a DataFrame with each column corresponding to one of the requested attributes. The scope for this annotation is \"token\".</p> <p>Parameters:     conc (Concordance or ConcordanceSubset): The concordance data.     args (dict): A dictionary of arguments with the following keys:         - spacy_model (str): The spaCy model to use for POS tagging. Default is \"en_core_web_sm\".         - tokens_attribute (str): The token attribute to use for POS tagging. Default is \"word\".         - spacy_attributes (list of str): A list of spaCy token attributes to retrieve.           Allowed values are \"pos_\", \"tag_\", \"morph\", \"dep_\", \"ent_type_\". Default is [\"pos_\"].</p> <p>Returns:     pd.DataFrame: A DataFrame indexed by token IDs with one column per requested attribute.</p>"},{"location":"algorithms/all_algorithms/#file-annotate_tf_idfpy","title":"File: annotate_tf_idf.py","text":""},{"location":"algorithms/all_algorithms/#annotate_tf_idf","title":"<code>annotate_tf_idf</code>","text":"<p>Annotates a concordance with TF-IDF vectors computed for each line based on tokens in a specified window.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it. - args (dict): Arguments include:     - tokens_attribute (str): The token attribute to use for creating line texts. Default is 'word'.     - exclude_values_attribute (str, optional): The attribute to filter out specific values.     - exclude_values_list (list, optional): The list of values to exclude.     - window_start (int): The lower bound of the token window (inclusive). Default is -5.     - window_end (int): The upper bound of the token window (inclusive). Default is 5.     - include_node (bool): Whether to include the node token (offset 0). Default is True.</p> <p>Returns: - pd.Series: A Pandas Series indexed by concordance line IDs, containing the TF-IDF vectors for each line.</p>"},{"location":"algorithms/all_algorithms/#file-partition_openai_semanticpy","title":"File: partition_openai_semantic.py","text":""},{"location":"algorithms/all_algorithms/#clusteringresult","title":"<code>ClusteringResult</code>","text":"<p>No docstring provided.</p>"},{"location":"algorithms/all_algorithms/#partition_openai_semantic","title":"<code>partition_openai_semantic</code>","text":"<p>Sends a list of lines to OpenAI and requests clustering into <code>n_partitions</code> groups with labels, using structured outputs for guaranteed JSON schema adherence.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it. - args (dict): Arguments include:     - openai_api_key (str): The API key for OpenAI.     - n_partitions (int): The number of partitions/clusters to create. Default is 5.     - token_attr (str): The token attribute to use for creating line texts. Default is 'word'.     - model (str): The OpenAI model to use. Default is 'gpt-4o-2024-11-20'.     - introduction_line (str): Customizable prompt for the clustering task.</p> <p>Returns: - list: A list of dictionaries, where each dictionary contains:     - \"label\": The label of the cluster.     - \"line_ids\": A list of line IDs in the cluster.</p>"},{"location":"algorithms/all_algorithms/#file-partition_by_metadata_attributepy","title":"File: partition_by_metadata_attribute.py","text":""},{"location":"algorithms/all_algorithms/#partition_by_metadata_attribute","title":"<code>partition_by_metadata_attribute</code>","text":"<p>Partitions the concordance data based on a specified metadata attribute and groups the lines accordingly.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it. - args (dict): Arguments include:     - metadata_attribute (str): The metadata attribute to partition by (e.g., 'pos', 'speaker').     - sort_by_partition_size (bool): If True, partitions will be sorted by size in descending order.     - sorted_values (List[Union[str, int]], optional): If provided, partitions will be sorted by these specific values.</p> <p>Returns: - dict: A dictionary containing:     - \"partitions\": A list of dictionaries, where each dictionary has:         - \"label\": The value of the metadata attribute for this partition.         - \"line_ids\": A list of line IDs that belong to this partition.</p>"},{"location":"algorithms/all_algorithms/#file-select_sort_wrapperpy","title":"File: select_sort_wrapper.py","text":""},{"location":"algorithms/all_algorithms/#select_by_sort","title":"<code>select_by_sort</code>","text":"<p>Selects lines based on sort keys obtained from the active_node's ordering_result['sort_keys'], using a comparison operator and a specified value.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters:     conc (Union[Concordance, ConcordanceSubset]): The concordance or subset of data.     args (dict): Arguments include:         - comparison_operator (str): The comparison operator ('==', '&lt;=', '&gt;=', '&lt;', '&gt;'). Default is \"==\".         - value (number): The value to compare the sort keys against. Default is 0.</p> <p>Returns:     dict: A dictionary containing:         - \"selected_lines\": A sorted list of selected line IDs.         - \"line_count\": The total number of selected lines.</p>"},{"location":"algorithms/all_algorithms/#file-sort_randompy","title":"File: sort_random.py","text":""},{"location":"algorithms/all_algorithms/#sort_random","title":"<code>sort_random</code>","text":"<p>Sorts lines pseudo-randomly while ensuring that given a specific seed, any pair of line_ids always appear in the same relative order regardless of what other line_ids are present.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it. - **kwargs: Arguments defined dynamically in the schema.</p> <p>Returns: - dict: A dictionary containing:     - \"sort_keys\": A mapping from line IDs to their stable pseudo-random ranks.</p>"},{"location":"algorithms/all_algorithms/#file-sort_by_token_attributepy","title":"File: sort_by_token_attribute.py","text":""},{"location":"algorithms/all_algorithms/#sort_by_token_attribute","title":"<code>sort_by_token_attribute</code>","text":"<p>Sorts the concordance lines by a specified token-level attribute. It supports sorting by a single token at a given offset (sorting_scope=\"token\"), or by the whole left context (sorting_scope=\"left\") or whole right context (sorting_scope=\"right\").</p> <p>For left context, tokens are joined from right to left (i.e. starting with offset -1, then -2, etc.).</p> <p>Locale-specific sorting is attempted via pyicu; if unavailable, plain Unicode sorting is used. Additionally, outputs token_spans for the tokens used for sorting.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters:   - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it.   - args (dict): Arguments include:       - tokens_attribute (str): The token attribute to sort by (e.g., \"word\", \"lemma\", \"pos\"). Default is \"word\".       - sorting_scope (str): Specifies which context to use for sorting:                            \"token\" for a single token at the given offset (default),                            \"left\" for the entire left context (tokens with offset &lt; 0 joined from right to left),                            \"right\" for the entire right context (tokens with offset &gt; 0 joined with a space).       - offset (int): The offset value to filter tokens by when sorting_scope==\"token\". Default is 0.       - case_sensitive (bool): If True, performs a case-sensitive sort. Default is False.       - reverse (bool): If True, sort in descending order. Default is False.       - backwards (bool): If True, reverses the string (e.g., for right-to-left sorting). Default is False.       - locale_str (str): ICU locale string for language-specific sorting. Default is \"en\".</p> <p>Returns:   dict: A dictionary containing:       - \"sort_keys\": A mapping from line IDs to their sorted ranks.       - \"token_spans\": A DataFrame with columns:             line_id, start_id_in_line, end_id_in_line, category, weight.       The token_spans represent the span (min and max id_in_line) of the tokens used for sorting.</p>"},{"location":"algorithms/all_algorithms/#file-partition_by_embeddingspy","title":"File: partition_by_embeddings.py","text":""},{"location":"algorithms/all_algorithms/#partition_by_embeddings","title":"<code>partition_by_embeddings</code>","text":"<p>Partitions lines based on embeddings stored in a concordance metadata column using clustering algorithms.</p> <p>Supports Agglomerative Clustering and K-Means.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it. - args (dict): Arguments include:     - embeddings_column (str): The metadata column containing embeddings for each line.     - n_partitions (int): The number of partitions/clusters to create. Default is 5.     - metric (str): The metric to compute distances between embeddings (only for Agglomerative). Default is \"cosine\".     - linkage (str): The linkage criterion for Agglomerative Clustering. Default is \"average\".     - method (str): The clustering method (\"agglomerative\" or \"kmeans\"). Default is \"agglomerative\".</p> <p>Returns: - list: A list of dictionaries, where each dictionary contains:     - \"label\": The label of the cluster.     - \"line_ids\": A list of line IDs in the cluster.</p>"},{"location":"algorithms/all_algorithms/#file-annotate_sentence_transformerspy","title":"File: annotate_sentence_transformers.py","text":""},{"location":"algorithms/all_algorithms/#annotate_sentence_transformers","title":"<code>annotate_sentence_transformers</code>","text":"<p>Annotates a concordance with embeddings generated by a Sentence Transformer model.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Concordance or ConcordanceSubset): The concordance data. - args (dict): Arguments include:     - tokens_attribute (str): The positional attribute to extract tokens from (e.g., \"word\"). Default is \"word\".     - window_start (int, optional): The lower bound of the window (inclusive). Default is None (entire line).     - window_end (int, optional): The upper bound of the window (inclusive). Default is None (entire line).     - model_name (str): The name of the pretrained Sentence Transformer model. Default is \"all-MiniLM-L6-v2\".</p> <p>Returns: - pd.Series: A Pandas Series indexed by concordance line IDs, containing the embeddings for each line.</p>"},{"location":"algorithms/all_algorithms/#file-annotate_spacy_embeddingspy","title":"File: annotate_spacy_embeddings.py","text":""},{"location":"algorithms/all_algorithms/#annotate_spacy_embeddings","title":"<code>annotate_spacy_embeddings</code>","text":"<p>Annotates a concordance with embeddings generated by averaging spaCy word embeddings for tokens within a specified window.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it. - args (dict): Arguments include:     - spacy_model (str): The spaCy model to use. Default is \"en_core_web_md\".     - tokens_attribute (str): The token attribute to use for creating line texts. Default is \"word\".     - exclude_values_attribute (str, optional): The attribute to filter out specific values.     - exclude_values_list (list, optional): The list of values to exclude.     - window_start (int): The lower bound of the token window (inclusive). Default is -5.     - window_end (int): The upper bound of the token window (inclusive). Default is 5.     - include_node (bool): Whether to include the node token (offset 0). Default is True.</p> <p>Returns: - pd.Series: A Pandas Series indexed by concordance line IDs, containing the averaged embeddings for each line.</p>"},{"location":"algorithms/all_algorithms/#file-select_by_metadata_attributepy","title":"File: select_by_metadata_attribute.py","text":""},{"location":"algorithms/all_algorithms/#select_by_metadata_attribute","title":"<code>select_by_metadata_attribute</code>","text":"<p>Selects concordance lines based on a specified metadata attribute comparing it to a target value.</p> <p>When the target value is a list, only equality is used (the metadata value must equal one of the list items). When the target value is a single numeric value, a comparison operator (one of \"==\", \"&lt;\", \"&lt;=\", \"&gt;\", \"&gt;=\") can be provided. For string values, only equality is supported (with optional regex matching and case sensitivity).</p> <p>Parameters:     conc (Concordance or ConcordanceSubset): The concordance object.     args (dict): Arguments include:         - metadata_attribute (str): The metadata attribute to filter on.         - value (str, number, or list of str/number): The value (or list of values) to compare against.         - operator (str, optional): Comparison operator for numeric comparisons. One of \"==\", \"&lt;\", \"&lt;=\", \"&gt;\", \"&gt;=\".                                     Default is \"==\".                                     This parameter is ignored if a list is provided or if the value is a string.         - regex (bool, optional): If True, for string values use regex matching (only with equality). Default is False.         - case_sensitive (bool, optional): If True, perform case-sensitive matching for strings. Default is False.         - negative (bool, optional): If True, invert the selection. Default is False.</p> <p>Returns:     dict: A dictionary containing:         - \"selected_lines\": A sorted list of line IDs for which the metadata attribute meets the condition.</p>"},{"location":"algorithms/all_algorithms/#file-select_slotpy","title":"File: select_slot.py","text":""},{"location":"algorithms/all_algorithms/#select_slot","title":"<code>select_slot</code>","text":"<p>Selects the appropriate offset column based on the slot_id.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The concordance or subset of data. - args (dict): Arguments include:     - slot_id (int): The slot identifier used to generate the offset column name.</p> <p>Returns: - dict: A dictionary containing:     - \"slot_to_use\": The slot ID being selected.     - \"selected_lines\": A list of all line IDs in the concordance.     - \"line_count\": The total number of lines.</p>"},{"location":"algorithms/all_algorithms/#file-select_by_token_attributepy","title":"File: select_by_token_attribute.py","text":""},{"location":"algorithms/all_algorithms/#select_by_token_attribute","title":"<code>select_by_token_attribute</code>","text":"<p>Selects lines based on a positional attribute at a given offset.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters:   - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it.   - **kwargs: Arguments defined dynamically in the schema.</p> <p>Returns:   - dict: A dictionary containing:       - \"selected_lines\": A list of line IDs where the condition is met.</p>"},{"location":"algorithms/all_algorithms/#file-select_manualpy","title":"File: select_manual.py","text":""},{"location":"algorithms/all_algorithms/#select_manual","title":"<code>select_manual</code>","text":"<p>Manually selects lines into a subset by providing a list of line IDs or by specifying groups (by labels or numbers) from the active node's grouping result. Groups may be partitions or clusters. In case of clusters (which may be nested), the entire grouping structure is traversed recursively to collect all groups that match the given identifiers.</p> <p>Additionally, this algorithm ensures that only lines that are present in the current node's selected_lines (or its closest ancestor that has this attribute) are allowed.</p> <p>Args:     conc (Union[Concordance, ConcordanceSubset]): The concordance or its subset.     args (dict): Arguments include:         - line_ids (list, optional): A list of specific line IDs to include in the subset.         - groups (list, optional): A list of group identifiers (either integers or strings) that           refer to groups (partitions or clusters) in the grouping_result.</p> <p>Returns:     dict: A dictionary containing:         - \"selected_lines\": A sorted list of unique selected line IDs.         - \"line_count\": The total number of selected lines.</p>"},{"location":"algorithms/all_algorithms/#file-select_set_operationpy","title":"File: select_set_operation.py","text":""},{"location":"algorithms/all_algorithms/#select_set_operation","title":"<code>select_set_operation</code>","text":"<p>Performs a set operation (union, intersection, difference, disjunctive union, complement) on the sets of lines from specified nodes in the analysis tree.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The concordance or subset of data. - args (dict): Arguments include:     - operation_type (str): Type of set operation ('union', 'intersection', 'difference',                             'disjunctive union', 'complement').     - nodes (list): A list of nodes to retrieve selected lines from.</p> <p>Returns: - dict: A dictionary containing:     - \"selected_lines\": A sorted list of line IDs resulting from the set operation.     - \"line_count\": The total number of selected lines.</p>"},{"location":"algorithms/all_algorithms/#file-select_weighted_sample_by_metadatapy","title":"File: select_weighted_sample_by_metadata.py","text":""},{"location":"algorithms/all_algorithms/#select_weighted_sample_by_metadata","title":"<code>select_weighted_sample_by_metadata</code>","text":"<p>Selects a weighted sample of lines based on the distribution of a specified metadata attribute.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Concordance or ConcordanceSubset): The concordance data. - args (dict): Arguments include:     - metadata_attribute (str): The metadata attribute to stratify by (e.g., 'genre', 'speaker').     - sample_size (int): The total number of lines to sample.     - seed (int, optional): Random seed for reproducibility. Default is None.</p> <p>Returns: - dict: A dictionary containing:     - 'selected_lines': A list of line IDs that have been sampled.     - 'line_count': The total number of lines sampled.</p>"},{"location":"algorithms/all_algorithms/#file-partition_ngramspy","title":"File: partition_ngrams.py","text":""},{"location":"algorithms/all_algorithms/#partition_ngrams","title":"<code>partition_ngrams</code>","text":"<p>Extracts ngram patterns from specified positions within each line and partitions the concordance according to the frequency of these patterns.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it. - args (dict): Arguments include:     - positions (List[int]): The list of positions (offsets) to extract for the ngram pattern.     - tokens_attribute (str): The positional attribute to search within (e.g., 'word'). Default is 'word'.     - case_sensitive (bool): If True, the search is case-sensitive. Default is False.</p> <p>Returns: - list: A list of dictionaries, where each dictionary has:     - \"label\": The ngram pattern (as a string).     - \"line_ids\": A list of line IDs associated with the pattern.</p>"},{"location":"algorithms/all_algorithms/#file-rank_kwic_grouperpy","title":"File: rank_kwic_grouper.py","text":""},{"location":"algorithms/all_algorithms/#rank_kwic_grouper","title":"<code>rank_kwic_grouper</code>","text":"<p>Ranks lines based on the count of a search term within a specific positional attribute column within a given window (KWIC). Additionally, returns token spans for matching tokens.</p> <p>Args are dynamically validated and extracted from the schema.</p> <p>Parameters: - conc (Union[Concordance, ConcordanceSubset]): The full concordance or a subset of it. - **kwargs: Arguments defined dynamically in the schema.</p> <p>Returns: - dict: A dictionary containing:     - \"rank_keys\": A mapping from line IDs to their ranking values based on the count of the search term.     - \"token_spans\": A DataFrame with columns:           id, line_id, start_id_in_line, end_id_in_line, category, weight.       Here, category is \"A\", weight is 1, and since each span is one token long,       start_id_in_line equals end_id_in_line (an inclusive index).</p>"},{"location":"algorithms/annotation/","title":"Annotation Algorithms","text":""},{"location":"algorithms/annotation/#annotate-association-scores","title":"Annotate Association Scores","text":"<p>Path: <code>flexiconc/algorithms/annotate_association_scores.py</code></p> <p>Description:</p> <p>Computes association scores from two frequency lists (concordance and whole corpus).</p> <p>Arguments:</p> Name Type Description corpus_frequency_list string Name of the corpus frequency list registered in <code>conc.resources</code>. concordance_frequency_list string Name of the concordance frequency list registered in <code>conc.resources</code>. token_attribute string Token attribute column common to both lists. ignore_case boolean Lowercase all string tokens before matching Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"corpus_frequency_list\": {\n      \"type\": \"string\",\n      \"description\": \"Name of the *corpus* frequency list registered in `conc.resources`.\",\n      \"x-eval\": \"dict(enum=conc.resources.list('frequency_list'))\"\n    },\n    \"concordance_frequency_list\": {\n      \"type\": \"string\",\n      \"description\": \"Name of the *concordance* frequency list registered in `conc.resources`.\",\n      \"x-eval\": \"dict(enum=conc.resources.list('frequency_list'))\"\n    },\n    \"token_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"Token attribute column common to both lists.\"\n    },\n    \"ignore_case\": {\n      \"type\": \"boolean\",\n      \"description\": \"Lowercase all string tokens before matching\",\n      \"default\": true\n    }\n  },\n  \"required\": [\n    \"corpus_frequency_list\",\n    \"concordance_frequency_list\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/annotation/#token-level-frequency-list","title":"Token-level Frequency List","text":"<p>Path: <code>flexiconc/algorithms/annotate_concordance_frequency_list.py</code></p> <p>Description:</p> <p>Aggregates token frequencies within an optional window and returns a FlexiConc frequency-list resource.</p> <p>Arguments:</p> Name Type Description token_attribute string Token attribute to count types for. window_start ['integer', 'null'] Lower bound of token window (inclusive). Null means unbounded. window_end ['integer', 'null'] Upper bound of token window (inclusive). Null means unbounded. include_node boolean Include the node token (offset 0) in the counting window. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"token_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"Token attribute to count types for.\",\n      \"default\": \"word\",\n      \"x-eval\": \"dict(enum=list(set(conc.tokens.columns) - {'id_in_line', 'line_id', 'offset'}))\"\n    },\n    \"window_start\": {\n      \"type\": [\n        \"integer\",\n        \"null\"\n      ],\n      \"description\": \"Lower bound of token window (inclusive). Null means unbounded.\",\n      \"default\": null,\n      \"x-eval\": \"dict(minimum=min(conc.tokens['offset']))\"\n    },\n    \"window_end\": {\n      \"type\": [\n        \"integer\",\n        \"null\"\n      ],\n      \"description\": \"Upper bound of token window (inclusive). Null means unbounded.\",\n      \"default\": null,\n      \"x-eval\": \"dict(maximum=max(conc.tokens['offset']))\"\n    },\n    \"include_node\": {\n      \"type\": \"boolean\",\n      \"description\": \"Include the node token (offset 0) in the counting window.\",\n      \"default\": false\n    }\n  },\n  \"required\": [\n    \"token_attribute\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/annotation/#annotate-with-sentence-transformers","title":"Annotate with Sentence Transformers","text":"<p>Path: <code>flexiconc/algorithms/annotate_sentence_transformers.py</code></p> <p>Description:</p> <p>Generates embeddings for each concordance line (or part of it) using a Sentence Transformer model. Allows selection of tokens within a specified window and based on a specified token attribute.</p> <p>Arguments:</p> Name Type Description tokens_attribute string The positional attribute to extract tokens from (e.g., 'word'). window_start integer The lower bound of the window (inclusive). If None, uses the entire line. window_end integer The upper bound of the window (inclusive). If None, uses the entire line. model_name string The name of the pretrained Sentence Transformer model. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"tokens_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The positional attribute to extract tokens from (e.g., 'word').\",\n      \"default\": \"word\",\n      \"x-eval\": \"dict(enum=list(set(conc.tokens.columns) - {'id_in_line', 'line_id', 'offset'}))\"\n    },\n    \"window_start\": {\n      \"type\": \"integer\",\n      \"description\": \"The lower bound of the window (inclusive). If None, uses the entire line.\",\n      \"x-eval\": \"dict(minimum=min(conc.tokens['offset']))\"\n    },\n    \"window_end\": {\n      \"type\": \"integer\",\n      \"description\": \"The upper bound of the window (inclusive). If None, uses the entire line.\",\n      \"x-eval\": \"dict(maximum=max(conc.tokens['offset']))\"\n    },\n    \"model_name\": {\n      \"type\": \"string\",\n      \"description\": \"The name of the pretrained Sentence Transformer model.\",\n      \"default\": \"all-MiniLM-L6-v2\"\n    }\n  },\n  \"required\": []\n}\n</code></pre>"},{"location":"algorithms/annotation/#annotate-with-spacy-embeddings","title":"Annotate with SpaCy Embeddings","text":"<p>Path: <code>flexiconc/algorithms/annotate_spacy_embeddings.py</code></p> <p>Description:</p> <p>Generates averaged spaCy word embeddings for tokens within a specified window.</p> <p>Arguments:</p> Name Type Description spacy_model string The spaCy model to use. tokens_attribute string The token attribute to use for creating line texts. exclude_values_attribute string The attribute to filter out specific values. exclude_values_list array The list of values to exclude. window_start integer The lower bound of the token window (inclusive). window_end integer The upper bound of the token window (inclusive). include_node boolean Whether to include the node token (offset 0). Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"spacy_model\": {\n      \"type\": \"string\",\n      \"description\": \"The spaCy model to use.\",\n      \"default\": \"en_core_web_md\"\n    },\n    \"tokens_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The token attribute to use for creating line texts.\",\n      \"default\": \"word\",\n      \"x-eval\": \"dict(enum=list(set(conc.tokens.columns) - {'id_in_line', 'line_id', 'offset'}))\"\n    },\n    \"exclude_values_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The attribute to filter out specific values.\"\n    },\n    \"exclude_values_list\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"The list of values to exclude.\"\n    },\n    \"window_start\": {\n      \"type\": \"integer\",\n      \"description\": \"The lower bound of the token window (inclusive).\",\n      \"default\": -5,\n      \"x-eval\": \"dict(minimum=min(conc.tokens['offset']))\"\n    },\n    \"window_end\": {\n      \"type\": \"integer\",\n      \"description\": \"The upper bound of the token window (inclusive).\",\n      \"default\": 5,\n      \"x-eval\": \"dict(maximum=max(conc.tokens['offset']))\"\n    },\n    \"include_node\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to include the node token (offset 0).\",\n      \"default\": true\n    }\n  },\n  \"required\": [\n    \"spacy_model\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/annotation/#annotate-with-spacy-pos-tags","title":"Annotate with spaCy POS tags","text":"<p>Path: <code>flexiconc/algorithms/annotate_spacy_pos.py</code></p> <p>Description:</p> <p>Annotates tokens with spaCy part-of-speech tags or related tag information using a specified spaCy model. The spacy_attributes parameter is always a list, so multiple annotations can be retrieved simultaneously.</p> <p>Arguments:</p> Name Type Description spacy_model string The spaCy model to use for POS tagging. tokens_attribute string The token attribute to use for POS tagging. spacy_attributes array A list of spaCy token attributes to retrieve for annotation. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"spacy_model\": {\n      \"type\": \"string\",\n      \"description\": \"The spaCy model to use for POS tagging.\",\n      \"default\": \"en_core_web_sm\"\n    },\n    \"tokens_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The token attribute to use for POS tagging.\",\n      \"default\": \"word\",\n      \"x-eval\": \"dict(enum=list(set(conc.tokens.columns) - {'id_in_line', 'line_id', 'offset'}))\"\n    },\n    \"spacy_attributes\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\",\n        \"enum\": [\n          \"lemma_\",\n          \"pos_\",\n          \"tag_\",\n          \"morph\",\n          \"dep_\",\n          \"ent_type_\"\n        ]\n      },\n      \"description\": \"A list of spaCy token attributes to retrieve for annotation.\",\n      \"default\": [\n        \"pos_\"\n      ]\n    }\n  },\n  \"required\": [\n    \"spacy_model\",\n    \"spacy_attributes\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/annotation/#annotate-with-tf-idf","title":"Annotate with TF-IDF","text":"<p>Path: <code>flexiconc/algorithms/annotate_tf_idf.py</code></p> <p>Description:</p> <p>Computes TF-IDF vectors for each line based on tokens in a specified window.</p> <p>Arguments:</p> Name Type Description tokens_attribute string The token attribute to use for creating line texts. exclude_values_attribute ['string'] The attribute to filter out specific values. exclude_values_list array The list of values to exclude. window_start integer The lower bound of the token window (inclusive). window_end integer The upper bound of the token window (inclusive). include_node boolean Whether to include the node token (offset 0). Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"tokens_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The token attribute to use for creating line texts.\",\n      \"default\": \"word\",\n      \"x-eval\": \"dict(enum=list(set(conc.tokens.columns) - {'id_in_line', 'line_id', 'offset'}))\"\n    },\n    \"exclude_values_attribute\": {\n      \"type\": [\n        \"string\"\n      ],\n      \"description\": \"The attribute to filter out specific values.\"\n    },\n    \"exclude_values_list\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"The list of values to exclude.\"\n    },\n    \"window_start\": {\n      \"type\": \"integer\",\n      \"description\": \"The lower bound of the token window (inclusive).\",\n      \"default\": -5,\n      \"x-eval\": \"dict(minimum=min(conc.tokens['offset']))\"\n    },\n    \"window_end\": {\n      \"type\": \"integer\",\n      \"description\": \"The upper bound of the token window (inclusive).\",\n      \"default\": 5,\n      \"x-eval\": \"dict(maximum=max(conc.tokens['offset']))\"\n    },\n    \"include_node\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to include the node token (offset 0).\",\n      \"default\": true\n    }\n  },\n  \"required\": []\n}\n</code></pre>"},{"location":"algorithms/partitioning/","title":"Partitioning Algorithms","text":""},{"location":"algorithms/partitioning/#flat-clustering-by-embeddings","title":"Flat Clustering by Embeddings","text":"<p>Path: <code>flexiconc/algorithms/partition_by_embeddings.py</code></p> <p>Description:</p> <p>Partitions lines based on embeddings stored in a concordance metadata column using clustering algorithms (Agglomerative Clustering or K-Means). Supports customizable distance metrics and linkage criteria.</p> <p>Arguments:</p> Name Type Description embeddings_column string The metadata column containing embeddings for each line. n_partitions integer The number of partitions/clusters to create. metric string The metric to compute distances between embeddings (used for Agglomerative Clustering only). linkage string The linkage criterion for Agglomerative Clustering (used only when method is 'agglomerative'). method string The clustering method to use ('agglomerative' or 'kmeans'). Default is 'agglomerative'. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"embeddings_column\": {\n      \"type\": \"string\",\n      \"description\": \"The metadata column containing embeddings for each line.\",\n      \"x-eval\": \"dict(enum=[col for col in list(conc.metadata.columns) if (hasattr(conc.metadata[col].iloc[0], '__iter__') and not isinstance(conc.metadata[col].iloc[0], str) and all(isinstance(x, __import__('numbers').Number) for x in conc.metadata[col].iloc[0]))])\"\n    },\n    \"n_partitions\": {\n      \"type\": \"integer\",\n      \"description\": \"The number of partitions/clusters to create.\",\n      \"default\": 5,\n      \"x-eval\": \"dict(maximum=node.line_count)\"\n    },\n    \"metric\": {\n      \"type\": \"string\",\n      \"description\": \"The metric to compute distances between embeddings (used for Agglomerative Clustering only).\",\n      \"default\": \"cosine\"\n    },\n    \"linkage\": {\n      \"type\": \"string\",\n      \"description\": \"The linkage criterion for Agglomerative Clustering (used only when method is 'agglomerative').\",\n      \"default\": \"average\"\n    },\n    \"method\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"agglomerative\",\n        \"kmeans\"\n      ],\n      \"description\": \"The clustering method to use ('agglomerative' or 'kmeans'). Default is 'agglomerative'.\",\n      \"default\": \"kmeans\"\n    }\n  },\n  \"required\": [\n    \"embeddings_column\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/partitioning/#partition-by-metadata-attribute","title":"Partition by Metadata Attribute","text":"<p>Path: <code>flexiconc/algorithms/partition_by_metadata_attribute.py</code></p> <p>Description:</p> <p>Partitions the concordance lines based on a specified metadata attribute and groups the data by the values of this attribute.</p> <p>Arguments:</p> Name Type Description metadata_attribute string The metadata attribute to partition by (e.g., 'text_id', 'speaker'). sort_by_partition_size boolean If True, partitions will be sorted by size in descending order. sorted_values array If provided, partitions will be sorted by these specific values. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"metadata_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The metadata attribute to partition by (e.g., 'text_id', 'speaker').\",\n      \"x-eval\": \"dict(enum=list(set(conc.metadata.columns) - {'line_id'}))\"\n    },\n    \"sort_by_partition_size\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, partitions will be sorted by size in descending order.\",\n      \"default\": true\n    },\n    \"sorted_values\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": [\n          \"string\",\n          \"number\"\n        ]\n      },\n      \"description\": \"If provided, partitions will be sorted by these specific values.\"\n    }\n  },\n  \"required\": [\n    \"metadata_attribute\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/partitioning/#partition-by-ngrams","title":"Partition by Ngrams","text":"<p>Path: <code>flexiconc/algorithms/partition_ngrams.py</code></p> <p>Description:</p> <p>Extracts ngram patterns from specified positions and partitions the concordance according to their frequency in the concordance lines. Compare Anthony's (2018) KWIC Patterns and subsequent work.</p> <p>Arguments:</p> Name Type Description positions array The list of positions (offsets) to extract for the ngram pattern. tokens_attribute string The positional attribute to search within (e.g., 'word'). case_sensitive boolean If True, the search is case-sensitive. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"positions\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"integer\"\n      },\n      \"description\": \"The list of positions (offsets) to extract for the ngram pattern.\"\n    },\n    \"tokens_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The positional attribute to search within (e.g., 'word').\",\n      \"default\": \"word\",\n      \"x-eval\": \"dict(enum=list(set(conc.tokens.columns) - {'id_in_line', 'line_id', 'offset'}))\"\n    },\n    \"case_sensitive\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, the search is case-sensitive.\",\n      \"default\": false\n    }\n  },\n  \"required\": [\n    \"positions\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/ranking/","title":"Ranking Algorithms","text":""},{"location":"algorithms/ranking/#collocation-ranker","title":"Collocation Ranker","text":"<p>Path: <code>flexiconc/algorithms/rank_by_collocations.py</code></p> <p>Description:</p> <p>Ranks lines by the sum (or count) of association-measure scores within a window.</p> <p>Arguments:</p> Name Type Description scores_list string Name of a scores resource registered in <code>conc.resources</code>. token_attribute string Token attribute shared by the scores table and the concordance tokens. score_column string Numeric column in the scores table to use. top_n ['integer', 'null'] Number of top collocates to take into account. method string 'sum' = add up scores, 'count' = count top-N collocates. window_start ['integer', 'null'] Lower bound of token window (inclusive). window_end ['integer', 'null'] Upper bound of token window (inclusive). positive_filter object Only include tokens matching these {attribute: [values]} pairs. negative_filter object Exclude tokens matching these {attribute: [values]} pairs. include_node boolean Include the node token (offset 0) in the window. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"scores_list\": {\n      \"type\": \"string\",\n      \"description\": \"Name of a *scores* resource registered in `conc.resources`.\",\n      \"x-eval\": \"dict(enum=conc.resources.list('scores'))\"\n    },\n    \"token_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"Token attribute shared by the scores table and the concordance tokens.\",\n      \"default\": \"word\",\n      \"x-eval\": \"dict(enum=list(set(conc.tokens.columns) - {'id_in_line', 'line_id', 'offset'}))\"\n    },\n    \"score_column\": {\n      \"type\": \"string\",\n      \"description\": \"Numeric column in the scores table to use.\",\n      \"default\": \"log_local_MI\"\n    },\n    \"top_n\": {\n      \"type\": [\n        \"integer\",\n        \"null\"\n      ],\n      \"description\": \"Number of top collocates to take into account.\",\n      \"default\": null\n    },\n    \"method\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"sum\",\n        \"count\"\n      ],\n      \"description\": \"'sum' = add up scores, 'count' = count top-N collocates.\",\n      \"default\": \"sum\"\n    },\n    \"window_start\": {\n      \"type\": [\n        \"integer\",\n        \"null\"\n      ],\n      \"description\": \"Lower bound of token window (inclusive).\",\n      \"default\": null,\n      \"x-eval\": \"dict(minimum=min(conc.tokens['offset']))\"\n    },\n    \"window_end\": {\n      \"type\": [\n        \"integer\",\n        \"null\"\n      ],\n      \"description\": \"Upper bound of token window (inclusive).\",\n      \"default\": null,\n      \"x-eval\": \"dict(maximum=max(conc.tokens['offset']))\"\n    },\n    \"positive_filter\": {\n      \"type\": \"object\",\n      \"description\": \"Only include tokens matching these {attribute: [values]} pairs.\"\n    },\n    \"negative_filter\": {\n      \"type\": \"object\",\n      \"description\": \"Exclude tokens matching these {attribute: [values]} pairs.\"\n    },\n    \"include_node\": {\n      \"type\": \"boolean\",\n      \"description\": \"Include the node token (offset 0) in the window.\",\n      \"default\": false\n    }\n  },\n  \"required\": [\n    \"scores_list\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/ranking/#kwic-grouper-ranker","title":"KWIC Grouper Ranker","text":"<p>Path: <code>flexiconc/algorithms/rank_kwic_grouper.py</code></p> <p>Description:</p> <p>Ranks lines based on the count of search terms in a specified token attribute within a window.</p> <p>Arguments:</p> Name Type Description search_terms array A list of terms to search for within the tokens. tokens_attribute string The token attribute to search within (e.g., 'word'). mode string Matching strategy for search_terms case_sensitive boolean If True, the search is case-sensitive. include_node boolean If True, include node-level tokens in the search. window_start integer The lower bound of the window (offset range). window_end integer The upper bound of the window (offset range). count_types boolean If True, count unique types within each line; otherwise, count all matches. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"search_terms\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"A list of terms to search for within the tokens.\"\n    },\n    \"tokens_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The token attribute to search within (e.g., 'word').\",\n      \"default\": \"word\",\n      \"x-eval\": \"dict(enum=list(set(conc.tokens.columns) - {'id_in_line', 'line_id', 'offset'}))\"\n    },\n    \"mode\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"literal\",\n        \"regex\",\n        \"cqp\"\n      ],\n      \"description\": \"Matching strategy for search_terms\",\n      \"default\": \"literal\"\n    },\n    \"case_sensitive\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, the search is case-sensitive.\",\n      \"default\": false\n    },\n    \"include_node\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, include node-level tokens in the search.\",\n      \"default\": false\n    },\n    \"window_start\": {\n      \"type\": \"integer\",\n      \"description\": \"The lower bound of the window (offset range).\",\n      \"x-eval\": \"dict(minimum=min(conc.tokens['offset']))\"\n    },\n    \"window_end\": {\n      \"type\": \"integer\",\n      \"description\": \"The upper bound of the window (offset range).\",\n      \"x-eval\": \"dict(maximum=max(conc.tokens['offset']))\"\n    },\n    \"count_types\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, count unique types within each line; otherwise, count all matches.\",\n      \"default\": true\n    }\n  },\n  \"required\": [\n    \"search_terms\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/ranking/#rank-by-number-of-rare-words","title":"Rank by Number of Rare Words","text":"<p>Path: <code>flexiconc/algorithms/rank_number_of_rare_words.py</code></p> <p>Description:</p> <p>Ranks lines by their count of rare words.</p> <p>Arguments:</p> Name Type Description p_attr string Token attribute to look up in the frequency list freq_list string Name of a registered frequency list frequency_type string Type of frequency to use: raw frequency ('f'), relative frequency ('rel_f'), or instances per million words ('pmw'). threshold number Frequency threshold below which tokens count as rare rank_threshold integer Rank threshold above which tokens count as rare window_start integer Lower bound of the token-offset window (inclusive) window_end integer Upper bound of the token-offset window (inclusive) case_sensitive boolean Match tokens against the frequency list case-sensitively positive boolean If True, the score is the raw count of rare tokens (more-rare \u2192 higher score). If False (default), score is the negative count so lines with fewer rare words rank higher. ignore_attrs object Mapping of token attrs \u2192 list of values to ignore Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"p_attr\": {\n      \"type\": \"string\",\n      \"description\": \"Token attribute to look up in the frequency list\",\n      \"default\": \"word\",\n      \"x-eval\": \"dict(enum=list(set(conc.tokens.columns) - {'id_in_line','line_id','offset'}))\"\n    },\n    \"freq_list\": {\n      \"type\": \"string\",\n      \"description\": \"Name of a registered frequency list\",\n      \"x-eval\": \"dict(enum=conc.resources.list('frequency_list'))\"\n    },\n    \"frequency_type\": {\n      \"type\": \"string\",\n      \"description\": \"Type of frequency to use: raw frequency ('f'), relative frequency ('rel_f'), or instances per million words ('pmw').\",\n      \"enum\": [\n        \"f\",\n        \"rel_f\",\n        \"pmw\"\n      ],\n      \"default\": \"pmw\"\n    },\n    \"threshold\": {\n      \"type\": \"number\",\n      \"description\": \"Frequency threshold below which tokens count as rare\"\n    },\n    \"rank_threshold\": {\n      \"type\": \"integer\",\n      \"description\": \"Rank threshold above which tokens count as rare\"\n    },\n    \"window_start\": {\n      \"type\": \"integer\",\n      \"description\": \"Lower bound of the token-offset window (inclusive)\"\n    },\n    \"window_end\": {\n      \"type\": \"integer\",\n      \"description\": \"Upper bound of the token-offset window (inclusive)\"\n    },\n    \"case_sensitive\": {\n      \"type\": \"boolean\",\n      \"description\": \"Match tokens against the frequency list case-sensitively\",\n      \"default\": false\n    },\n    \"positive\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, the score is the raw count of rare tokens (more-rare \\u2192 higher score). If False (default), score is the *negative* count so lines with fewer rare words rank higher.\",\n      \"default\": false\n    },\n    \"ignore_attrs\": {\n      \"type\": \"object\",\n      \"description\": \"Mapping of token attrs \\u2192 list of values to ignore\",\n      \"default\": {},\n      \"x-eval\": \"dict(propertyNames={'enum': list(set(conc.tokens.columns) - {'id_in_line','line_id','offset'})})\",\n      \"additionalProperties\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": [\n            \"string\",\n            \"number\",\n            \"boolean\"\n          ]\n        }\n      }\n    }\n  },\n  \"required\": [\n    \"p_attr\",\n    \"freq_list\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/selecting/","title":"Selecting Algorithms","text":""},{"location":"algorithms/selecting/#select-by-metadata-attribute","title":"Select by Metadata Attribute","text":"<p>Path: <code>flexiconc/algorithms/select_by_metadata_attribute.py</code></p> <p>Description:</p> <p>Selects lines based on whether a specified metadata attribute compares to a given target value. If a list is provided as the target value, membership is tested using equality. For a single numeric value, a comparison operator (==, &lt;, &lt;=, &gt;, &gt;=) can be specified. For strings, only equality (with optional regex matching and case sensitivity) is supported.</p> <p>Arguments:</p> Name Type Description metadata_attribute string The metadata attribute to filter on. value ['string', 'number', 'array'] The value to compare against, or a list of acceptable values. When a list is provided, only equality is used. operator string The comparison operator for numeric comparisons. Only allowed for single numeric values. Default is '=='. regex boolean If True, use regex matching for string comparisons (only with equality). Default is False. case_sensitive boolean If True, perform case-sensitive matching for strings. Default is False. negative boolean If True, invert the selection. Default is False. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"metadata_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The metadata attribute to filter on.\",\n      \"x-eval\": \"dict(enum=list(set(conc.metadata.columns) - {'line_id'}))\"\n    },\n    \"value\": {\n      \"type\": [\n        \"string\",\n        \"number\",\n        \"array\"\n      ],\n      \"description\": \"The value to compare against, or a list of acceptable values. When a list is provided, only equality is used.\"\n    },\n    \"operator\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"==\",\n        \"&lt;\",\n        \"&lt;=\",\n        \"&gt;\",\n        \"&gt;=\"\n      ],\n      \"description\": \"The comparison operator for numeric comparisons. Only allowed for single numeric values. Default is '=='.\",\n      \"default\": \"==\"\n    },\n    \"regex\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, use regex matching for string comparisons (only with equality). Default is False.\",\n      \"default\": false\n    },\n    \"case_sensitive\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, perform case-sensitive matching for strings. Default is False.\",\n      \"default\": false\n    },\n    \"negative\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, invert the selection. Default is False.\",\n      \"default\": false\n    }\n  },\n  \"required\": [\n    \"metadata_attribute\",\n    \"value\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/selecting/#select-by-rank","title":"Select by Rank","text":"<p>Path: <code>flexiconc/algorithms/select_rank_wrapper.py</code></p> <p>Description:</p> <p>Selects lines based on rank values obtained from the ranking keys in the ordering_result['rank_keys'] of the current node, by default by the first ranking key. </p> <p>Arguments:</p> Name Type Description ranking_column string The ranking column to use for selection. comparison_operator string The comparison operator to use for the ranking scores. value number The numeric value to compare the ranking scores against. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"ranking_column\": {\n      \"type\": \"string\",\n      \"description\": \"The ranking column to use for selection.\",\n      \"x-eval\": \"dict(enum=[f'{x}: {node.algorithms[\\\"ordering\\\"][x][\\\"algorithm_name\\\"]}' for x in list(node.ordering_result['rank_keys'])], default=[f'{x}: {node.algorithms[\\\"ordering\\\"][x][\\\"algorithm_name\\\"]}' for x in list(node.ordering_result['rank_keys'])][0])\"\n    },\n    \"comparison_operator\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"==\",\n        \"&lt;=\",\n        \"&gt;=\",\n        \"&lt;\",\n        \"&gt;\"\n      ],\n      \"description\": \"The comparison operator to use for the ranking scores.\",\n      \"default\": \"==\"\n    },\n    \"value\": {\n      \"type\": \"number\",\n      \"description\": \"The numeric value to compare the ranking scores against.\",\n      \"default\": 0\n    }\n  },\n  \"required\": []\n}\n</code></pre>"},{"location":"algorithms/selecting/#select-by-token-level-numeric-attribute","title":"Select by Token-Level Numeric Attribute","text":"<p>Path: <code>flexiconc/algorithms/select_by_token_numeric_value.py</code></p> <p>Description:</p> <p>Selects lines based on a token-level attribute using numeric comparison at a given offset. If a list is provided for 'value', only equality comparison is performed.</p> <p>Arguments:</p> Name Type Description value ['number', 'array'] The numeric value(s) to compare against. If a list is provided, only equality comparison is supported. tokens_attribute string The token-level attribute to check. offset integer The token offset to check. comparison_operator string The comparison operator to use for numeric values. Ignored if 'value' is a list. negative boolean If True, inverts the selection. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"value\": {\n      \"type\": [\n        \"number\",\n        \"array\"\n      ],\n      \"items\": {\n        \"type\": \"number\"\n      },\n      \"description\": \"The numeric value(s) to compare against. If a list is provided, only equality comparison is supported.\",\n      \"default\": 0\n    },\n    \"tokens_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The token-level attribute to check.\",\n      \"x-eval\": \"dict(enum=[col for col in list(conc.tokens.columns) if col not in {'id_in_line', 'line_id', 'offset'} and ('int' in str(conc.tokens[col].dtype) or 'float' in str(conc.tokens[col].dtype))])\"\n    },\n    \"offset\": {\n      \"type\": \"integer\",\n      \"description\": \"The token offset to check.\",\n      \"default\": 0,\n      \"x-eval\": \"dict(minimum=min(conc.tokens['offset']), maximum=max(conc.tokens['offset']))\"\n    },\n    \"comparison_operator\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"==\",\n        \"&lt;\",\n        \"&gt;\",\n        \"&lt;=\",\n        \"&gt;=\"\n      ],\n      \"description\": \"The comparison operator to use for numeric values. Ignored if 'value' is a list.\",\n      \"default\": \"==\"\n    },\n    \"negative\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, inverts the selection.\",\n      \"default\": false\n    }\n  },\n  \"required\": [\n    \"value\",\n    \"tokens_attribute\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/selecting/#select-by-token-level-string-attribute","title":"Select by Token-Level String Attribute","text":"<p>Path: <code>flexiconc/algorithms/select_by_token_string.py</code></p> <p>Description:</p> <p>Selects lines based on a token-level attribute (string matching) at a given offset. Supports regex and case sensitivity. The search_terms argument is a list of strings to match against.</p> <p>Arguments:</p> Name Type Description search_terms array The list of string values to match against. tokens_attribute string The token attribute to check (e.g., 'word'). offset integer The token offset to check. case_sensitive boolean If True, performs a case-sensitive match. regex boolean If True, uses regex matching. negative boolean If True, inverts the selection. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"search_terms\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"The list of string values to match against.\",\n      \"default\": []\n    },\n    \"tokens_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The token attribute to check (e.g., 'word').\",\n      \"default\": \"word\",\n      \"x-eval\": \"dict(enum=list(set(conc.tokens.columns) - {'id_in_line', 'line_id', 'offset'}))\"\n    },\n    \"offset\": {\n      \"type\": \"integer\",\n      \"description\": \"The token offset to check.\",\n      \"default\": 0,\n      \"x-eval\": \"dict(minimum=min(conc.tokens['offset']), maximum=max(conc.tokens['offset']))\"\n    },\n    \"case_sensitive\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, performs a case-sensitive match.\",\n      \"default\": false\n    },\n    \"regex\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, uses regex matching.\",\n      \"default\": false\n    },\n    \"negative\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, inverts the selection.\",\n      \"default\": false\n    }\n  },\n  \"required\": [\n    \"search_terms\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/selecting/#manual-line-selection","title":"Manual Line Selection","text":"<p>Path: <code>flexiconc/algorithms/select_manual.py</code></p> <p>Description:</p> <p>Manually selects lines into a subset by specifying line IDs or groups (partitions or clusters) from the active node's grouping result. Additionally, ensures selection is restricted to allowed lines.</p> <p>Arguments:</p> Name Type Description line_ids array A list of specific line IDs to include in the subset. groups array A list of group identifiers (by label or number) to include lines from. For clusters, groups may be nested, and all matching groups in the hierarchy will be used. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"line_ids\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"integer\"\n      },\n      \"description\": \"A list of specific line IDs to include in the subset.\"\n    },\n    \"groups\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": [\n          \"string\",\n          \"integer\"\n        ]\n      },\n      \"description\": \"A list of group identifiers (by label or number) to include lines from. For clusters, groups may be nested, and all matching groups in the hierarchy will be used.\"\n    }\n  },\n  \"required\": []\n}\n</code></pre>"},{"location":"algorithms/selecting/#random-sample","title":"Random Sample","text":"<p>Path: <code>flexiconc/algorithms/select_random.py</code></p> <p>Description:</p> <p>Selects a random sample of lines from the concordance, optionally using a seed.</p> <p>Arguments:</p> Name Type Description sample_size integer The number of lines to sample. seed integer The seed for random number generation. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"sample_size\": {\n      \"type\": \"integer\",\n      \"description\": \"The number of lines to sample.\",\n      \"minimum\": 1,\n      \"x-eval\": \"dict(maximum=node.line_count)\"\n    },\n    \"seed\": {\n      \"type\": \"integer\",\n      \"description\": \"The seed for random number generation.\",\n      \"default\": 42\n    }\n  },\n  \"required\": [\n    \"sample_size\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/selecting/#select-slot","title":"Select Slot","text":"<p>Path: <code>flexiconc/algorithms/select_slot.py</code></p> <p>Description:</p> <p>Selects the slot to work with.</p> <p>Arguments:</p> Name Type Description slot_id integer The slot identifier to select. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"slot_id\": {\n      \"type\": \"integer\",\n      \"description\": \"The slot identifier to select.\",\n      \"x-eval\": \"dict(enum=list(set(conc.matches['slot'])))\"\n    }\n  },\n  \"required\": [\n    \"slot_id\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/selecting/#select-weighted-sample-by-metadata","title":"Select Weighted Sample by Metadata","text":"<p>Path: <code>flexiconc/algorithms/select_weighted_sample_by_metadata.py</code></p> <p>Description:</p> <p>Selects a weighted sample of lines based on the distribution of a specified metadata attribute.</p> <p>Arguments:</p> Name Type Description metadata_attribute string The metadata attribute to stratify by (e.g., 'text_id', 'speaker'). sample_size integer The total number of lines to sample. seed ['integer'] An optional seed for generating the pseudo-random order. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"metadata_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The metadata attribute to stratify by (e.g., 'text_id', 'speaker').\",\n      \"x-eval\": \"dict(enum=list(set(conc.metadata.columns) - {'line_id'}))\"\n    },\n    \"sample_size\": {\n      \"type\": \"integer\",\n      \"description\": \"The total number of lines to sample.\",\n      \"minimum\": 1,\n      \"x-eval\": \"dict(maximum=node.line_count)\"\n    },\n    \"seed\": {\n      \"type\": [\n        \"integer\"\n      ],\n      \"description\": \"An optional seed for generating the pseudo-random order.\",\n      \"default\": 42\n    }\n  },\n  \"required\": [\n    \"metadata_attribute\",\n    \"sample_size\"\n  ]\n}\n</code></pre>"},{"location":"algorithms/sorting/","title":"Sorting Algorithms","text":""},{"location":"algorithms/sorting/#sort-by-corpus-position","title":"Sort by Corpus Position","text":"<p>Path: <code>flexiconc/algorithms/sort_by_corpus_position.py</code></p> <p>Description:</p> <p>Sorts the concordance lines by their line_id, which corresponds to their position in the corpus.</p> <p>Arguments:</p> <p>No arguments defined.</p> Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"required\": []\n}\n</code></pre>"},{"location":"algorithms/sorting/#sort-by-token-level-attribute","title":"Sort by Token-Level Attribute","text":"<p>Path: <code>flexiconc/algorithms/sort_by_token_attribute.py</code></p> <p>Description:</p> <p>Sorts the concordance lines by the given token-level attribute using locale-specific sorting (default 'en'). Supports sorting by a single token at a given offset, or by whole left/right context by joining tokens. When sorting by left context, tokens are joined from right to left. Optionally reverses strings for right-to-left sorting.</p> <p>Arguments:</p> Name Type Description tokens_attribute string The token attribute to sort by. sorting_scope string Specifies which context to use for sorting: 'token' for a single token at the given offset, 'left' for the entire left context (joined from right to left), or 'right' for the entire right context. offset integer The offset value to filter tokens by when sorting_scope is 'token'. case_sensitive boolean If True, performs a case-sensitive sort. reverse boolean If True, sort in descending order. backwards boolean If True, reverses the string (e.g., for right-to-left sorting). locale_str string ICU locale string for language-specific sorting. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"tokens_attribute\": {\n      \"type\": \"string\",\n      \"description\": \"The token attribute to sort by.\",\n      \"default\": \"word\",\n      \"x-eval\": \"dict(enum=list(set(conc.tokens.columns) - {'id_in_line', 'line_id', 'offset'}))\"\n    },\n    \"sorting_scope\": {\n      \"type\": \"string\",\n      \"description\": \"Specifies which context to use for sorting: 'token' for a single token at the given offset, 'left' for the entire left context (joined from right to left), or 'right' for the entire right context.\",\n      \"default\": \"token\",\n      \"enum\": [\n        \"token\",\n        \"left\",\n        \"right\"\n      ]\n    },\n    \"offset\": {\n      \"type\": \"integer\",\n      \"description\": \"The offset value to filter tokens by when sorting_scope is 'token'.\",\n      \"default\": 0,\n      \"x-eval\": \"dict(minimum=min(conc.tokens['offset']), maximum=max(conc.tokens['offset']))\"\n    },\n    \"case_sensitive\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, performs a case-sensitive sort.\",\n      \"default\": false\n    },\n    \"reverse\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, sort in descending order.\",\n      \"default\": false\n    },\n    \"backwards\": {\n      \"type\": \"boolean\",\n      \"description\": \"If True, reverses the string (e.g., for right-to-left sorting).\",\n      \"default\": false\n    },\n    \"locale_str\": {\n      \"type\": \"string\",\n      \"description\": \"ICU locale string for language-specific sorting.\",\n      \"default\": \"en\"\n    }\n  },\n  \"required\": []\n}\n</code></pre>"},{"location":"algorithms/sorting/#random-sort","title":"Random Sort","text":"<p>Path: <code>flexiconc/algorithms/sort_random.py</code></p> <p>Description:</p> <p>Sorts lines in a pseudo-random but stable manner. Given a seed, any pair of line_ids always appear in the same relative order, independent of the presence of other lines.</p> <p>Arguments:</p> Name Type Description seed integer An optional seed for generating the pseudo-random order. Show full JSON schema <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"seed\": {\n      \"type\": \"integer\",\n      \"description\": \"An optional seed for generating the pseudo-random order.\",\n      \"default\": 42\n    }\n  },\n  \"required\": []\n}\n</code></pre>"}]}